---
title: "Sentiment Analysis FINAL"
author: "Joost Bloos"
date: "07/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Install packages:
#Source: https://trinkerrstuff.wordpress.com/my-r-packages/qdap/

#if (!require("pacman")) install.packages("pacman")
#pacman::p_load(sentimentr, dplyr, magrittr)
#install.packages("devtools")
#install_github("trinker/qdapDictionaries")
#install_github("trinker/qdapRegex")
#install_github("trinker/qdapTools")
#install_github("trinker/qdap")
#install.packages("quanteda")
#install.packages("sentimentr")
#install.packages("ndjson")
#install.packages("NLP")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("tm")
#install.packages("corpus")
#install.packages("syuzhet")
#install.packages("plotly")
#install.packages("wordcloud")

```


```{r}
library(devtools)
library(tm)
library(qdap)
library(sentimentr)
library(ndjson)
library(corpus)
library(syuzhet)
library(tidyr)
library(dplyr)
library(quanteda)
library(ggplot2)
library(plotly)
library(wordcloud)

#a good package, also takes into account  negative words and amplifiers
#see: http://www.inside-r.org/packages/cran/qdap/docs/polarity
```

```{r}
#getwd()
#setwd("C:/Ryerson University - Capstone project/Module 2/EIEEE - Large dataset/Combined")
```


```{r}
#Read in original data set May 2020
data_set_may <- read.csv("corona_tweets_59 May 2020", header = T, sep = ",")

#take a sample of 1,000, set seed to replicate results across several analysis of methods:
set.seed(1000)
rawData <- data_set_may[sample(nrow(data_set_may), size = 1000), ]
#write.csv(rawData,'rawData.csv')

str(rawData)

```

```{r}
#create a corpus:
importdocs = corpus(rawData, text_field = 'text')
```

```{r}
#preprocessing of data
importdocs <- gsub("'", "", importdocs)  # remove apostrophes
importdocs <- gsub("[[:punct:]]", " ", importdocs)  # replace punctuation with space
importdocs <- gsub("[[:cntrl:]]", " ", importdocs)  # replace control characters with space
importdocs <- gsub("^[[:space:]]+", "", importdocs) # remove whitespace at beginning of documents
importdocs <- gsub("[[:space:]]+$", "", importdocs) # remove whitespace at end of documents
importdocs <- tolower(importdocs)


# CLEANING TWEETS

importdocs=gsub("&amp", "", importdocs)

importdocs = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", importdocs)
importdocs = gsub("@\\w+", "", importdocs)

importdocs = gsub("[[:digit:]]", "", importdocs)
importdocs = gsub("http\\w+", "", importdocs)
importdocs = gsub("[ \t]{2,}", "", importdocs)
importdocs = gsub("^\\s+|\\s+$", "", importdocs)

importdocs <- iconv(importdocs, "UTF-8", "ASCII", sub="")

str(importdocs)
```


```{r}
mycorpus <- get_sentences(importdocs)
mysentiment <- sentiment(mycorpus)
mysentiment
```

```{r}
# run overall score, result overall neutral to perhaps moderate positive
summary(mysentiment$sentiment)
```

```{r}
#results expressed in histogram

qplot(mysentiment$sentiment,   geom="histogram",binwidth=0.1,main="Review Sentiment Histogram")
```

```{r}
#source: https://www.programmingr.com/sentiment-analysis/

#returns the individual words along with their polarity strength and counts.
t = extract_sentiment_terms(mycorpus) 
attributes(t)$count
```

```{r}
#show positive and negative word use:
head(t,20)
```

```{r}
# The emotion() function returns the rate of emotion per sentence. A data frame is returned by this function and of interest to us are the two columns: emotion type and emotion. Emotion indicates the strength of emotion present in the sentence.
emotion(mycorpus[1:2])
```






```{r}
# graph with emotional valence, what is explanation. Note to self: look up
plot(mysentiment)
```

```{r}
#integrate sentiment score into updated dataset
sentimentResultMay2020 <- rawData
sentimentResultMay2020$sentiment_score = mysentiment$sentiment
str(sentimentResultMay2020)
```

```{r}
#identify text for max (positive) sentiment score
max(mysentiment$sentiment)
maxSentiment <- sentimentResultMay2020[which.max(sentimentResultMay2020$sentiment_score),]
maxSentiment$text
```

```{r}
#identify text for min sentiment score
min(mysentiment$sentiment)
minSentiment <- sentimentResultMay2020[which.min(sentimentResultMay2020$sentiment_score),]
minSentiment$text
```

```{r}
#write sentiment score to original dataset
write.csv(sentimentResultMay2020,'sentimentResultMay2020.csv')
```

```{r}
#Source: https://www.tabvizexplorer.com/sentiment-analysis-using-r-and-twitter/


#score the emotions on each tweet with syuzhet as it breaks emotion into 10 different categories.
# Emotions for each tweet using NRC dictionary
emotions <- get_nrc_sentiment(importdocs)
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
```

```{r}
# visualize results to what type of emotions are dominant in the tweets
# Visualize the emotions from NRC sentiments
library(plotly)
p <- plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
  layout(xaxis=list(title=""), showlegend=FALSE,
         title="Emotion Type for Covid related hastags (source: IEEE)")
p

#Here we see majority of the people are discussing positive about Covid.
```

```{r}
# Create comparison word cloud data
wordcloud_tweet = c(
  paste(importdocs[emotions$anger > 0], collapse=" "),
  paste(importdocs[emotions$anticipation > 0], collapse=" "),
  paste(importdocs[emotions$disgust > 0], collapse=" "),
  paste(importdocs[emotions$fear > 0], collapse=" "),
  paste(importdocs[emotions$joy > 0], collapse=" "),
  paste(importdocs[emotions$sadness > 0], collapse=" "),
  paste(importdocs[emotions$surprise > 0], collapse=" "),
  paste(importdocs[emotions$trust > 0], collapse=" ")
)
```



```{r}
# create corpus
corpus = Corpus(VectorSource(wordcloud_tweet))
```

```{r}
# remove punctuation, convert every word in lower case and remove stop words

corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
corpus = tm_map(corpus, stemDocument)

#warning: transformation drops documents
```

```{r}
# create document term matrix
tdm = TermDocumentMatrix(corpus)
```

```{r}
# convert as matrix
tdm = as.matrix(tdm)
tdmnew <- tdm[nchar(rownames(tdm)) < 11,]
```

```{r}
#Graph presents which word contributes to which emotion.

# column name binding
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
colnames(tdmnew) <- colnames(tdm)
comparison.cloud(tdmnew, random.order=FALSE,
                 colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"),
                 title.size=1, max.words=200, scale=c(2.4, 0.4),rot.per=0.4)
```





